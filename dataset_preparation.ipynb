{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5fc57a-5275-412d-9cf4-386375ebc6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "######## PARSING THE DATA #########\n",
    "# we are cleaning up the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7984ddd-4882-4812-8d36-4134c3fcfe18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# disclaimer: you can't run all of the code in this notebook, this is just all of the code combined that we used in data preparation\n",
    "# If you want to use all the code, make separate notebooks for each section separated by a comment cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e4375b-7f72-4199-beeb-3a980cebecdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1a2381-0cea-4dd0-b7d8-62ec2bbcf471",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/srv/scratch/ALL/DATA/NR-DBIND.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6df9fc-a69e-4d55-a52b-b63ef810ba78",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ID'] = df['ID'].astype(int).astype(str)\n",
    "df = df[df['p_binding_type'].isin(['pIC50', 'pKi'])] ## We only want p_binding_type to be pIC50 and pKi\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0ca740-2720-41ea-ab5c-3fd252afd4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only choose the necessary columns\n",
    "df = df[['ID', 'accession', 'smiles', 'CHEMBLID', 'p_binding_value']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8a6186-bfb9-49f8-952d-5959f1e969a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1. group by accession + smiles - all rows with the same accession and smiles are put into a group\n",
    "## 2. aggregate with median - for each group take the median\n",
    "## 3. reset the index to normal numbering\n",
    "df = df.groupby(['accession', 'smiles']).agg('median').reset_index()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb4022f-5fef-44a3-bb2a-014e0cf33769",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only choose rows where p_binding_value is not NaN\n",
    "df = df[df['p_binding_value'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1bf295-51a8-4645-9ca8-dfd687676e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save df to csv\n",
    "df.to_csv('parsed_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712f3eb1-3edb-4b9b-8727-bd5dbeb71285",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "######## ADDING AMINO ACID SEQUENCES ######### (not as important as I had thought it would be)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881626ec-5e75-4963-a230-eb9984f2fe60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('parsed_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83256991-b57c-46fc-9196-9cc516b00685",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = df['accession'].unique()\n",
    "for i in a:\n",
    "    print(i)    #### this prints out the name of each protein in a new line ===> input to uniprot \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a04aca-9bbf-4c25-9d79-e3fecbcdd625",
   "metadata": {},
   "outputs": [],
   "source": [
    "######### after having gotten the fasta file from https://www.uniprot.org/uploadlists/   ===>\n",
    " \n",
    "    \n",
    "name = 'sequences.fasta' # I just named it that way\n",
    "\n",
    "def getSeq(filename):                 #### makes a dictionary of the ID of the protein ('accession') as key and the sequence as value\n",
    "    f = open(filename)\n",
    "    id2seq = {}\n",
    "    currkey = \"\"\n",
    "    for line in f:\n",
    "        if line.find(\">\") == 0:\n",
    "            currkey = (line[1:].split(\"|\")[1])\n",
    "            id2seq[currkey] = \"\"\n",
    "        else:            \n",
    "            id2seq[currkey] = id2seq[currkey] + line.rstrip()\n",
    "    f.close()\n",
    "    return id2seq \n",
    "dictionary = getSeq(name)\n",
    "\n",
    "df[\"AAsequence\"] = df['accession'].map(dictionary)\n",
    "df.to_csv('NEWDATASET.csv', index = 'false')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a45f75-282c-4b3a-af78-647ed07d3d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "######## PREPARING THE DATA FOR PROTEINS ######## (crucial)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc8a607-db44-442d-8716-382d419fddfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### ypu need the 3D structures of all of the proteins\n",
    "### (nodes, edges, edge attributes, edge mapping)\n",
    "\n",
    "import pickle\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from pandas.core.frame import DataFrame\n",
    "from torch_geometric.utils import to_undirected\n",
    "\n",
    "aa_encoding = {\n",
    "    \"ala\": 0,\n",
    "    \"arg\": 1,\n",
    "    \"asn\": 2,\n",
    "    \"asp\": 3,\n",
    "    \"cys\": 4,\n",
    "    \"gln\": 5,\n",
    "    \"glu\": 6,\n",
    "    \"gly\": 7,\n",
    "    \"his\": 8,\n",
    "    \"ile\": 9,\n",
    "    \"leu\": 10,\n",
    "    \"lys\": 11,\n",
    "    \"met\": 12,\n",
    "    \"phe\": 13,\n",
    "    \"pro\": 14,\n",
    "    \"ser\": 15,\n",
    "    \"thr\": 16,\n",
    "    \"trp\": 17,\n",
    "    \"tyr\": 18,\n",
    "    \"val\": 19,\n",
    "}\n",
    "\n",
    "edge_type_encoding = {\"cnt\": 0, \"combi\": 1, \"hbond\": 2, \"pept\": 3, \"ovl\": 4}\n",
    "\n",
    "\n",
    "def onehot_encode(position: int, count: Optional[int] = 20):\n",
    "    \"\"\"One-hot encode position\n",
    "    Args:\n",
    "        position (int): Which entry to set to 1\n",
    "        count (Optional[int], optional): Max number of entries. Defaults to 20.\n",
    "    Returns:\n",
    "        [type]: [description]\n",
    "    \"\"\"\n",
    "    t = [0] * count\n",
    "    t[position] = 1\n",
    "    return t\n",
    "\n",
    "\n",
    "class ProteinEncoder:\n",
    "    def __init__(self, config: dict):\n",
    "        self.features = config[\"node_features\"]\n",
    "        self.edge_features = config[\"edge_features\"]\n",
    "\n",
    "    def encode_residue(self, residue: str) -> np.array:\n",
    "        \"\"\"Fully encode residue - one-hot and features\n",
    "\n",
    "        Args:\n",
    "            residue (str): One-letter residue name\n",
    "\n",
    "        Returns:\n",
    "            np.array: Concatenated features and one-hot encoding of residue name\n",
    "        \"\"\"\n",
    "        if residue.lower() not in aa_encoding:\n",
    "            return None\n",
    "        elif self.features == \"label\":\n",
    "            return aa_encoding[residue.lower()]\n",
    "        elif self.features == \"onehot\":\n",
    "            return onehot_encode(aa_encoding[residue.lower()])\n",
    "        else:\n",
    "            raise ValueError(\"Unknown features type!\")\n",
    "\n",
    "    def parse_sif(self, filename: str) -> Tuple[DataFrame, DataFrame]:\n",
    "        \"\"\"Parse a single sif file\n",
    "\n",
    "        Args:\n",
    "            filename (str): SIF file location\n",
    "\n",
    "        Returns:\n",
    "            Tuple[DataFrame, DataFrame]: nodes, edges DataFrames\n",
    "        \"\"\"\n",
    "        nodes = []\n",
    "        edges = []\n",
    "        with open(filename, \"r\") as file:\n",
    "            for line in file:\n",
    "                line = line.strip()\n",
    "                splitline = line.split()\n",
    "                if len(splitline) != 3:\n",
    "                    continue\n",
    "                node1, edgetype, node2 = splitline\n",
    "                node1split = node1.split(\":\")\n",
    "                node2split = node2.split(\":\")\n",
    "                if len(node1split) != 4:\n",
    "                    continue\n",
    "                if len(node2split) != 4:\n",
    "                    continue\n",
    "                chain1, resn1, x1, resaa1 = node1split\n",
    "                chain2, resn2, x2, resaa2 = node2split\n",
    "                if x1 != \"_\" or x2 != \"_\":\n",
    "                    continue\n",
    "                if resaa1.lower() not in aa_encoding or resaa2.lower() not in aa_encoding:\n",
    "                    continue\n",
    "                resn1 = int(resn1)\n",
    "                resn2 = int(resn2)\n",
    "                if resn1 == resn2:\n",
    "                    continue\n",
    "                edgesplit = edgetype.split(\":\")\n",
    "                if len(edgesplit) != 2:\n",
    "                    continue\n",
    "                node1 = {\"chain\": chain1, \"resn\": resn1, \"resaa\": resaa1}\n",
    "                node2 = {\"chain\": chain2, \"resn\": resn2, \"resaa\": resaa2}\n",
    "                edgetype, _ = edgesplit\n",
    "                edge = {\n",
    "                    \"resn1\": resn1,\n",
    "                    \"resn2\": resn2,\n",
    "                    \"type\": edgetype,\n",
    "                }\n",
    "                nodes.append(node1)\n",
    "                nodes.append(node2)\n",
    "                edges.append(edge)\n",
    "        nodes = pd.DataFrame(nodes).drop_duplicates()\n",
    "        nodes = nodes.sort_values(\"resn\").reset_index(drop=True).reset_index().set_index(\"resn\")\n",
    "        edges = pd.DataFrame(edges).drop_duplicates()\n",
    "        node_idx = nodes[\"index\"].to_dict()\n",
    "        edges[\"node1\"] = edges[\"resn1\"].apply(lambda x: node_idx[x])\n",
    "        edges[\"node2\"] = edges[\"resn2\"].apply(lambda x: node_idx[x])\n",
    "        return nodes, edges\n",
    "\n",
    "    def encode_nodes(self, nodes: pd.DataFrame) -> torch.Tensor:\n",
    "        \"\"\"Given dataframe of nodes create node features\n",
    "\n",
    "        Args:\n",
    "            nodes (pd.DataFrame): nodes dataframe from parse_sif\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Tensor of node features [n_nodes, *]\n",
    "        \"\"\"\n",
    "        nodes.drop_duplicates(inplace=True)\n",
    "        node_attr = [self.encode_residue(x) for x in nodes[\"resaa\"]]\n",
    "        node_attr = [x for x in node_attr if x is not None]\n",
    "        node_attr = np.asarray(node_attr)\n",
    "        if self.features == \"label\":\n",
    "            node_attr = torch.tensor(node_attr, dtype=torch.long)\n",
    "        else:\n",
    "            node_attr = torch.tensor(node_attr, dtype=torch.float32)\n",
    "        return node_attr\n",
    "\n",
    "    def encode_edges(self, edges: pd.DataFrame) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Given dataframe of edges, create edge index and edge attributes\n",
    "\n",
    "        Args:\n",
    "            edges (pd.DataFrame): edges dataframe from parse_sif\n",
    "\n",
    "        Returns:\n",
    "            Tuple[torch.Tensor, torch.Tensor]: edge index [2,n_edges], edge attributes [n_edges, *]\n",
    "        \"\"\"\n",
    "        edges.drop_duplicates(inplace=True)\n",
    "        edge_index = edges[[\"node1\", \"node2\"]].astype(int).values\n",
    "        edge_index = torch.tensor(edge_index, dtype=torch.long)\n",
    "        edge_index = edge_index.t().contiguous()\n",
    "        if self.edge_features == \"none\":\n",
    "            return edge_index, None\n",
    "        edge_features = edges[\"type\"].apply(lambda x: edge_type_encoding[x])\n",
    "        if self.edge_features == \"label\":\n",
    "            edge_features = torch.tensor(edge_features, dtype=torch.long)\n",
    "            edge_index, edge_features = to_undirected(edge_index, edge_features)\n",
    "            return edge_index, edge_features\n",
    "        elif self.edge_features == \"onehot\":\n",
    "            edge_features = edge_features.apply(onehot_encode, count=len(edge_type_encoding))\n",
    "            edge_features = torch.tensor(edge_features, dtype=torch.float)\n",
    "            edge_index, edge_features = to_undirected(edge_index, edge_features)\n",
    "            return edge_index, edge_features\n",
    "\n",
    "    def __call__(self, protein_sif: str) -> dict:\n",
    "        \"\"\"Fully process the protein\n",
    "\n",
    "        Args:\n",
    "            protein_sif (str): File location for sif file\n",
    "\n",
    "        Returns:\n",
    "            dict: standard format with x for node features, edge_index for edges etc\n",
    "        \"\"\"\n",
    "        nodes, edges = self.parse_sif(protein_sif)\n",
    "        node_attr = self.encode_nodes(nodes)\n",
    "        edge_index, edge_attr = self.encode_edges(edges)\n",
    "        return dict(x=node_attr, edge_index=edge_index, edge_attr=edge_attr, index_mapping=nodes[\"index\"].to_dict())\n",
    "\n",
    "\n",
    "def extract_name(protein_sif: str) -> str:\n",
    "    \"\"\"Extract the protein name from the sif filename\"\"\"\n",
    "    return protein_sif.split(\"/\")[-1].split(\"_\")[0]\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    proteins = pd.Series(list(snakemake.input.rins), name=\"sif\")\n",
    "    proteins = pd.DataFrame(proteins)\n",
    "    proteins[\"ID\"] = proteins[\"sif\"].apply(extract_name)\n",
    "    proteins.set_index(\"ID\", inplace=True)\n",
    "    prot_encoder = ProteinEncoder(snakemake.config)\n",
    "    proteins[\"data\"] = proteins[\"sif\"].apply(prot_encoder)\n",
    "    with open(snakemake.output.protein_pickle, \"wb\") as file:\n",
    "        pickle.dump(proteins, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f449f3-a90a-4940-920a-e5e185afade2",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_pickled = pd.read_pickle('protein_data_label_label.pkl')\n",
    "f_pickled.drop('sif', axis = 1)\n",
    "def get_accession(name):\n",
    "    name = name.split('/')[2]\n",
    "    name = name.split('-')[1]\n",
    "    return name\n",
    "f_pickled['sif'] = f_pickled['sif'].apply(get_accession)\n",
    "f_pickled['accession'] = f_pickled['sif']\n",
    "f_pickled.drop('sif', inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dad9b1f-d5d8-41f9-b57b-78d1fb291241",
   "metadata": {},
   "outputs": [],
   "source": [
    "proteins = open('new_dataset_proteins.pckl', 'wb') \n",
    "pickle.dump(f_pickled, proteins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cab714b-f844-4477-9647-a904d9d478d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "######## PREPARING THE DATA FOR DRUGS ######## (crucial)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1777b649-e3bc-4093-b401-4e1671bd2944",
   "metadata": {},
   "outputs": [],
   "source": [
    "### (nodes, edges, edge attributes (bond types))\n",
    "import pandas as pd                                       #### Importing some libraries, probably not all of them are important\n",
    "import numpy as np\n",
    "import torch\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import rdmolfiles, rdmolops\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import to_undirected\n",
    "import numpy as np\n",
    "import pickle \n",
    "\n",
    "dataset = pd.read_csv('NEWDATASET.csv')        ### opening the csv file where we have all our parsed data\n",
    "\n",
    "def smiles_to_torch(smiles: str) -> Data:               #### this is a function that takes in the smiles of drug \n",
    "    '''                                                 #### (example:CC(C)c1onc(c1COc2ccc(cc2)c3ccc4c(cccc4c3)C(=O)O)c5c(Cl)cccc5Cl)\n",
    "    Converts molecular smiles into torch data           #### it uses the torch library so i dont quite understand what its doing\n",
    "    '''\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if not mol:  # when rdkit fails to read a molecule it returns None\n",
    "      return np.nan\n",
    "    new_order = rdmolfiles.CanonicalRankAtoms(mol)\n",
    "    mol = rdmolops.RenumberAtoms(mol, new_order)\n",
    "    dictionary = {'SINGLE':0,'DOUBLE':1,'AROMATIC':2}   ### dictionary for bond types\n",
    "    edges = []\n",
    "    edge_atributes = []\n",
    "    for bond in mol.GetBonds():\n",
    "      start, end = bond.GetBeginAtomIdx(), bond.GetEndAtomIdx()\n",
    "      bond_type = str(bond.GetBondType())                        ### the addition of information about bond type\n",
    "      \n",
    "      if bond_type in dictionary:\n",
    "          type = dictionary[bond_type]\n",
    "      else:\n",
    "          type = 3\n",
    "      edge_atributes.append(type)\n",
    "      edges.append([start, end])\n",
    "\n",
    "    if not edges:  # If no edges (bonds) were found, exit (single ion etc)\n",
    "      return np.nan\n",
    "    atom_features = []\n",
    "    for atom in mol.GetAtoms():\n",
    "      atom_num = atom.GetAtomicNum()\n",
    "      atom_features.append(atom_num)\n",
    "\n",
    "    x = torch.tensor(atom_features, dtype=torch.long)\n",
    "    edge_index = torch.tensor(edges).t().contiguous()\n",
    "    edge_atributes = torch.tensor(edge_atributes, dtype = torch.long)\n",
    "\n",
    "    return dict(x=x, edge_index=edge_index, edge_atributes=edge_atributes)   #returns a dictionary of values (we are pairing the \n",
    "                                                                             #names of lists and list in tensor format of all nodes (atoms),\n",
    "                                                                             #list of edges (bonds), and list of bond types\n",
    "        \n",
    "dataset['data'] = dataset['smiles'].apply(smiles_to_torch)           ### we apply the smiles_to_torch function to every smiles\n",
    "dataset       ###  this just prints it out                           ### string, and putting into a new column called data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb57f3e-c57f-406e-bd94-12784eb88738",
   "metadata": {},
   "outputs": [],
   "source": [
    "drugs = open('new_dataset_drugs.pckl', 'wb') \n",
    "pickle.dump(dataset, drugs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b883e644-e451-4262-82eb-add6d4316f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "######## FINAL: MAKING A LIST THAT WOULD BE FED TO THE MODEL ##########\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fad6217-60df-4ba6-8638-2e9c09b2023c",
   "metadata": {},
   "outputs": [],
   "source": [
    "filehandler = open('../../../scratch/ALL/new_dataset_drugs.pckl', 'rb')                              ###\n",
    "drug = pickle.load(filehandler)\n",
    "filehandler = open('../../../DLDD/results/prepare_proteins/new_dataset_proteins.pckl', 'rb')         ### this is just where I saved them\n",
    "prot = pickle.load(filehandler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519b89d1-6cef-4896-ad9c-81b1dac31eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "prot_dict = {}\n",
    "for i in range(len(prot)):\n",
    "    row = prot.iloc[i]\n",
    "    prot_dict[row['accession']] = row['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c664745-9d17-45bc-b93e-4188c52a6838",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_for_model = []\n",
    "for name, row in drug.iterrows():\n",
    "    new_dict = {}\n",
    "    accession = row['accession']\n",
    "    data = row['data']\n",
    "    new_dict['drug_x'] = data['x'] \n",
    "    new_dict['drug_edge_index'] = data['edge_index']\n",
    "    new_dict['label'] = row['p_binding_value']\n",
    "    new_dict['protein_x'] = prot_dict[accession]['x']\n",
    "    new_dict['protein_edge_index'] = prot_dict[accession]['edge_index']\n",
    "    list_for_model.append(new_dict)\n",
    "list_for_model[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29744716-61d6-4b39-963f-5918a41388d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_list = open('final_list.pckl', 'wb') \n",
    "pickle.dump(list_for_model, final_list)                ### And finally we have a beautiful pickle file ready for the model\n",
    "                                                       ### CONGRATS !!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
